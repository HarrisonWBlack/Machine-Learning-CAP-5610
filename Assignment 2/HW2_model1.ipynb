{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_model1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarrisonWBlack/Machine-Learning-CAP-5610/blob/master/Assignment%202/HW2_model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iTkfOJQU0CVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Five layer, dropout, with batch normalization and data augmentation at 25 Epochs.** \n",
        "This model is the best of the five."
      ]
    },
    {
      "metadata": {
        "id": "PW9HYv5yfm_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "(training_images, training_labels), (testing_images, testing_labels) = cifar10.load_data()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 25\n",
        "augment_data = True\n",
        "\n",
        "training_labels = keras.utils.to_categorical(training_labels)\n",
        "testing_labels = keras.utils.to_categorical(testing_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10QKnrDMja54",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# .20 79.13\n",
        "# .30 79.22 elu - 76\n",
        "# .35 78.22\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=training_images.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout_rate))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vN_SUvYllw2e",
        "colab_type": "code",
        "outputId": "2a170c1e-1477-4dac-9766-f796a86a6e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1320
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "             optimizer=\"Adam\",\n",
        "             metrics=[\"accuracy\"])\n",
        "\n",
        "training_images = training_images.astype(\"float32\") / 255\n",
        "testing_images = testing_images.astype(\"float32\") / 255\n",
        "\n",
        "if not augment_data:\n",
        "  model.fit(training_images, training_labels,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=(testing_images, testing_labels),\n",
        "           shuffle=True)\n",
        "  \n",
        "else:\n",
        "  datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    zca_epsilon=1e-06,\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.,\n",
        "    zoom_range=0.,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode=\"nearest\",\n",
        "    cval=0.,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    rescale=None,\n",
        "    preprocessing_function=None,\n",
        "    data_format=None,\n",
        "    validation_split=0.0)\n",
        "  \n",
        "  datagen.fit(training_images)\n",
        "  model.fit_generator(datagen.flow(training_images, training_labels,\n",
        "                                  batch_size=batch_size),\n",
        "                     epochs=epochs,\n",
        "                     validation_data=(testing_images, testing_labels),\n",
        "                     workers=1)\n",
        "  "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 211us/sample - loss: 3.4457 - acc: 0.1326\n",
            "196/196 [==============================] - 52s 266ms/step - loss: 1.5643 - acc: 0.4444 - val_loss: 3.4484 - val_acc: 0.1326\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 2.8203 - acc: 0.2383\n",
            "196/196 [==============================] - 50s 254ms/step - loss: 1.2427 - acc: 0.5558 - val_loss: 2.8235 - val_acc: 0.2383\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 1s 94us/sample - loss: 1.3464 - acc: 0.5282\n",
            "196/196 [==============================] - 50s 258ms/step - loss: 1.1143 - acc: 0.6028 - val_loss: 1.3549 - val_acc: 0.5282\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 1s 139us/sample - loss: 1.1931 - acc: 0.5958\n",
            "196/196 [==============================] - 54s 274ms/step - loss: 1.0319 - acc: 0.6327 - val_loss: 1.1891 - val_acc: 0.5958\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 1s 92us/sample - loss: 1.1235 - acc: 0.6218\n",
            "196/196 [==============================] - 71s 360ms/step - loss: 0.9633 - acc: 0.6585 - val_loss: 1.1245 - val_acc: 0.6218\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 0.8174 - acc: 0.7147\n",
            "196/196 [==============================] - 50s 255ms/step - loss: 0.9118 - acc: 0.6783 - val_loss: 0.8171 - val_acc: 0.7147\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 1s 95us/sample - loss: 0.8052 - acc: 0.7231\n",
            "196/196 [==============================] - 50s 253ms/step - loss: 0.8723 - acc: 0.6938 - val_loss: 0.8013 - val_acc: 0.7231\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 1s 141us/sample - loss: 0.9435 - acc: 0.6845\n",
            "196/196 [==============================] - 52s 267ms/step - loss: 0.8432 - acc: 0.6997 - val_loss: 0.9388 - val_acc: 0.6845\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 1s 119us/sample - loss: 1.5089 - acc: 0.5637\n",
            "196/196 [==============================] - 53s 271ms/step - loss: 0.8106 - acc: 0.7131 - val_loss: 1.5003 - val_acc: 0.5637\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 1s 123us/sample - loss: 0.8527 - acc: 0.7061\n",
            "196/196 [==============================] - 53s 272ms/step - loss: 0.7889 - acc: 0.7225 - val_loss: 0.8465 - val_acc: 0.7061\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 1s 114us/sample - loss: 0.9353 - acc: 0.6835\n",
            "196/196 [==============================] - 53s 270ms/step - loss: 0.7673 - acc: 0.7294 - val_loss: 0.9266 - val_acc: 0.6835\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.8777 - acc: 0.7052\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 0.7495 - acc: 0.7360 - val_loss: 0.8726 - val_acc: 0.7052\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.9562 - acc: 0.6788\n",
            "196/196 [==============================] - 50s 255ms/step - loss: 0.7325 - acc: 0.7426 - val_loss: 0.9507 - val_acc: 0.6788\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.6486 - acc: 0.7804\n",
            "196/196 [==============================] - 50s 256ms/step - loss: 0.7112 - acc: 0.7484 - val_loss: 0.6464 - val_acc: 0.7804\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 1s 110us/sample - loss: 0.8163 - acc: 0.7231\n",
            "196/196 [==============================] - 50s 257ms/step - loss: 0.7026 - acc: 0.7533 - val_loss: 0.8093 - val_acc: 0.7231\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 1s 96us/sample - loss: 0.8005 - acc: 0.7287\n",
            "196/196 [==============================] - 50s 256ms/step - loss: 0.6922 - acc: 0.7566 - val_loss: 0.7962 - val_acc: 0.7287\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 1s 105us/sample - loss: 0.6863 - acc: 0.7586\n",
            "196/196 [==============================] - 50s 255ms/step - loss: 0.6762 - acc: 0.7614 - val_loss: 0.6817 - val_acc: 0.7586\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 1s 136us/sample - loss: 0.6786 - acc: 0.7661\n",
            "196/196 [==============================] - 70s 355ms/step - loss: 0.6724 - acc: 0.7620 - val_loss: 0.6761 - val_acc: 0.7661\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 1s 142us/sample - loss: 0.8301 - acc: 0.7253\n",
            "196/196 [==============================] - 86s 439ms/step - loss: 0.6605 - acc: 0.7688 - val_loss: 0.8251 - val_acc: 0.7253\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6625 - acc: 0.7696\n",
            "196/196 [==============================] - 89s 452ms/step - loss: 0.6500 - acc: 0.7713 - val_loss: 0.6620 - val_acc: 0.7696\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 0.6498 - acc: 0.7735\n",
            "196/196 [==============================] - 67s 339ms/step - loss: 0.6389 - acc: 0.7756 - val_loss: 0.6523 - val_acc: 0.7735\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 0.6800 - acc: 0.7705\n",
            "196/196 [==============================] - 52s 266ms/step - loss: 0.6317 - acc: 0.7767 - val_loss: 0.6828 - val_acc: 0.7705\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: 0.7386 - acc: 0.7518\n",
            "196/196 [==============================] - 50s 257ms/step - loss: 0.6241 - acc: 0.7802 - val_loss: 0.7392 - val_acc: 0.7518\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.8084 - acc: 0.7317\n",
            "196/196 [==============================] - 50s 255ms/step - loss: 0.6182 - acc: 0.7827 - val_loss: 0.8091 - val_acc: 0.7317\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 1s 96us/sample - loss: 0.6284 - acc: 0.7913\n",
            "196/196 [==============================] - 51s 259ms/step - loss: 0.6087 - acc: 0.7866 - val_loss: 0.6260 - val_acc: 0.7913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IThBqEX3rhI3",
        "colab_type": "code",
        "outputId": "c1b56361-ca00-4720-ab16-413df89ccb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(testing_images, testing_labels, verbose=1)\n",
        "print(\"Test loss: \", scores[0])\n",
        "print(\"Test accuracy: \", scores[1])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 249us/sample - loss: 0.6284 - acc: 0.7913\n",
            "Test loss:  0.6283716292381286\n",
            "Test accuracy:  0.7913\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}